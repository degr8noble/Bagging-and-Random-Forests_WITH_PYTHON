# Bagging-and-Random-Forests_WITH_PYTHON
In this notebook, we introduce the concept of bagging, which is shorthand for bootstrap aggregation, where random samples of the data are used to construct multiple learners (machine learning models). Since each learner only sees part of the data, each learner is less accurate than if it had been constructed over the full data set. Thus, each learner is known as a weak learner. A more powerful, meta-estimator is subsequently constructed by averaging over these many weak learners. The approach of constructing weak learners, and combining them into a more powerful estimator is at the heart of several, very powerful machine learning techniques, among which, the most popular one is the random forest.  In this notebook, we first introduce the formalism behind bagging, including a discussion of the concept of bootstrapping. Next, we move on to a discussion of the random forest algorithm, which will include its application to both classification and regression tasks.
